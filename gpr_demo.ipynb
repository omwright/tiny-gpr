{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f8263-d100-4f07-9b36-1bd4c42a1dbe",
   "metadata": {},
   "source": [
    "# Kriging from Scratch\n",
    "\n",
    "Kriging, or Gaussian process regression, uses Gaussian processes to predict values at unobserved locations by computing a weighted average of known values.\n",
    "\n",
    "A Gaussian process gives us a probability distribution over possible *functions*. It's a stochastic process in which every set of random variables is governed by a mutivariate Gaussian distribution. Why is this a useful way to model phenomena? In short, [lots of things are Gaussian](https://en.wikipedia.org/wiki/Central_limit_theorem) or can be well approximated as Gaussian, and lots quantities you might want to compute can be derived straightforwardly and in closed form when things are Gaussian.\n",
    "\n",
    "Kriging is especially powerful because it yields not just a single \"best fit\", but a distribution of functions that fit to the observations. This makes for a more robust approach, with which we can quantify predictive uncertainty and better inform downstream decision-making.\n",
    "\n",
    "We want to approximate a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ as best we can, using only a set of observations at points $\\{x_1, x_2, \\ldots, x_n\\}$ with corresponding values $\\{y_1, y_2, \\ldots, y_n\\}$. Kriging models this function as the realization of a Gaussian process\n",
    "$$f(x) \\sim \\mathcal{GP} \\left( m(x), k(x,x') \\right)$$\n",
    "where $m(x)$ is the mean function (typically set to zero or a constant), and $k(x,x')$ is the all-important kernel, or covariance function, which defines the similarity between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2987012-268e-4261-ac8f-876eae068276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.stats import norm\n",
    "\n",
    "random_seed = 19846\n",
    "rng = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c514c-7b74-48df-b8b7-4abee5d98383",
   "metadata": {},
   "source": [
    "Let's use a simple example. Suppose we want to model the function $f(x) = \\sin (\\frac{1}{2}\\pi x) - \\sin (\\pi x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d4958-1913-4e71-aae3-fae4bb509ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_model(x):\n",
    "    return np.sin(0.5*np.pi*x) - np.sin(np.pi*x)\n",
    "\n",
    "xt = np.linspace(0, 5, 200)\n",
    "f_x = signal_model(xt)\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\"font.family\":\"serif\"})\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=xt, y=f_x, color=\"black\", ax=ax)\n",
    "ax.set_ylim(-3,3)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(r'$f(x) = \\sin (\\frac{1}{2}\\pi x) - \\sin (\\pi x)$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39441e24-96c4-4dfc-9f74-ffddbe6d0829",
   "metadata": {},
   "source": [
    "Of course, we don't know the true function, we have only a collection of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8befa-aac8-441a-bb8b-7add70ea830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "xt = np.linspace(0, 5, 200)\n",
    "f_x = signal_model(xt)\n",
    "x = rng.uniform(0, 5, n_samples)\n",
    "y = signal_model(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=xt, y=f_x, color='black', linestyle='--', alpha=0.25, ax=ax, label='Ground truth');\n",
    "sns.scatterplot(x=x, y=y, color='red', ax=ax, zorder=3, label='Observations');\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(-3,3)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(r'$f(x) = \\sin (\\frac{1}{2}\\pi x) - \\sin (\\pi x)$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cede5f0-657a-46d2-a28b-dc1be9c049f3",
   "metadata": {},
   "source": [
    "What's the best way to estimate the function based on this handful of observations? For kriging, a necessary step is picking a kernel with which to define the Gaussian process. The selection of a kernel, or covariance function, is key. Through the kernel we specify a covariance matrix that completely specifies the distribution of functions. (This is because Gaussian processes are completely defined by their second-order statistics. Define the covariance function, define the behavior.)\n",
    "\n",
    "Selecting a kernel encodes prior knowledge. Different kernels induce different relationships between points: do you believe the function you're trying to model is smooth, noisy, linear, periodic, etc.? Poor kernel selection usually results in a poor estimate of the true function.\n",
    "\n",
    "A common kernel is the squared exponential, or [radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel):\n",
    "$$k(x, x') = \\exp \\left\\{ -\\frac{1}{2} (x - x')^2 \\right\\}$$\n",
    "\n",
    "With the kernel selected we can sample functions from the Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd9dcf-195b-41be-96c4-73da3287fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2=None, sigma_f=1, l=1):\n",
    "    \"\"\"Squared exponential kernel function.\"\"\"\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    sq_dist = (x1 - x2.T)**2\n",
    "    kernel = sigma_f * np.exp(-sq_dist / (2 * l**2))\n",
    "    return kernel\n",
    "\n",
    "n_instances = 25\n",
    "\n",
    "K = rbf_kernel(np.expand_dims(xt,1), sigma_f=2, l=0.5) # Covariance matrix\n",
    "m = np.zeros(200)\n",
    "\n",
    "f_prior = rng.multivariate_normal(m, K, n_instances)\n",
    "f_prior.shape\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_instances):\n",
    "    sns.lineplot(x=xt, y=f_prior[i,:], color='black', alpha=0.25, ax=ax);\n",
    "ax.set_title('Sample Functions from Prior Distribution', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf88a94-3b18-417e-8770-a0d808137106",
   "metadata": {},
   "source": [
    "Here are some other popoular kernels for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d484c-f3eb-4978-a881-12a51d03046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_3over2_kernel(x1, x2=None, sigma_f=1, l=1):\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    dist = np.abs(x1 - x2.T)\n",
    "    sqrt3 = np.sqrt(3)\n",
    "    return sigma_f**2 * (1 + (sqrt3*dist)/l) * np.exp(-sqrt3*dist/l)\n",
    "\n",
    "def brownian_kernel(x1, x2=None):\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    return np.minimum(x1, x2.T)\n",
    "\n",
    "def linear_kernel(x1, x2=None, sigma_m=1, sigma_b=1, c=0):\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    return sigma_b**2 + sigma_m**2 * np.dot(x1 - c, x2.T - c)\n",
    "\n",
    "K_matern = matern_3over2_kernel(np.expand_dims(xt,1))\n",
    "K_brownian = brownian_kernel(np.expand_dims(xt,1))\n",
    "K_linear = linear_kernel(np.expand_dims(xt,1), sigma_b=4)\n",
    "\n",
    "f_matern = rng.multivariate_normal(m, K_matern, n_instances)\n",
    "f_brownian = rng.multivariate_normal(m, K_brownian, n_instances)\n",
    "f_linear = rng.multivariate_normal(m, K_linear, n_instances)\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs = axs.flatten()\n",
    "for i in range(n_instances):\n",
    "    sns.lineplot(x=xt, y=f_prior[i,:], color='black', alpha=0.2, ax=axs[0]);\n",
    "    axs[0].set_ylim(-3,3)\n",
    "    axs[0].set_title(\"RBF Kernel\", fontsize=9)\n",
    "for i in range(n_instances):\n",
    "    sns.lineplot(x=xt, y=f_matern[i,:], color='black', alpha=0.2, ax=axs[1]);\n",
    "    axs[0].set_ylim(-3,3)\n",
    "    axs[1].set_title(r\"MatÃ©rn Kernel\", fontsize=9)\n",
    "for i in range(n_instances):\n",
    "    sns.lineplot(x=xt, y=f_brownian[i,:], color='black', alpha=0.2, ax=axs[2]);\n",
    "    axs[0].set_ylim(-3,3)\n",
    "    axs[2].set_title(\"Brownian Kernel\", fontsize=9)\n",
    "for i in range(n_instances):\n",
    "    sns.lineplot(x=xt, y=f_linear[i,:], color='black', alpha=0.2, ax=axs[3]);\n",
    "    axs[0].set_ylim(-3,3)\n",
    "    axs[3].set_title(\"Linear Kernel\", fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048a57b-fb65-4edf-a176-9897d0be8586",
   "metadata": {},
   "source": [
    "To make a prediction at a new point $x_*$, we compute\n",
    "$$\\hat{f}(x_*) = m(x_*) + k(x_*, X) \\left[ K(X, X) + \\sigma^2_n I \\right]^{-1} (y - m(X))$$\n",
    "$$\\text{Var}\\left( \\hat{f}(x_*) \\right) = k(x_*,x_*) - k(x_*,X) \\left[ K(X,X) + \\sigma^2_n I \\right]^{-1} k(X, x)$$\n",
    "where $X$ is the vector of observations $[x_1, x_2, \\ldots, x_n]^\\top$, $K$ the covariance matrix of observations as generated by the kernel $k$, and $\\sigma^2_n$ the noise variance (we'll get to noise shortly).\n",
    "\n",
    "Let's return to our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf85ff9-0dcf-4858-a48e-8d6fe19ee3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2=None, sigma_f=1, l=1):\n",
    "    # A more numerically stable implementation\n",
    "    if x2 is None:\n",
    "        x2 = x1\n",
    "    if x1.ndim == 1:\n",
    "        x1 = np.expand_dims(x1,1)\n",
    "        x2 = np.expand_dims(x2,1)\n",
    "    x1_norm = np.sum(x1**2, axis=1).reshape(-1, 1)\n",
    "    x2_norm = np.sum(x2**2, axis=1).reshape(1, -1)\n",
    "    sq_dist = x1_norm + x2_norm - 2.0 * np.dot(x1, x2.T)\n",
    "    sq_dist = np.maximum(sq_dist, 0.0)\n",
    "    kernel = sigma_f**2 * np.exp(-0.5 * sq_dist / l**2)\n",
    "    return kernel\n",
    "\n",
    "def gpr(x_train, y_train, x_test, kernel_func, sigma_n=0.0, **kernel_params):\n",
    "    \"\"\"Gaussian process regression\"\"\"\n",
    "    K = kernel_func(x_train, x_train, **kernel_params)\n",
    "    K = K + (sigma_n**2 + 1e-6) * np.eye(len(x_train))\n",
    "    K_s = kernel_func(x_train, x_test, **kernel_params)\n",
    "    K_ss = kernel_func(x_test, x_test, **kernel_params)\n",
    "\n",
    "    try:\n",
    "        # Try standard Cholesky approach first\n",
    "        L = np.linalg.cholesky(K)\n",
    "        alpha = np.linalg.solve(L.T, np.linalg.solve(L, y_train))\n",
    "        mu = K_s.T @ alpha\n",
    "        v = np.linalg.solve(L, K_s)\n",
    "        sigma = np.sqrt(np.diag(K_ss - v.T @ v))\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If Cholesky fails, use a slower but more stable approach\n",
    "        print(\"Warning: Matrix not positive semi-definite, using more robust method\")\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(K)\n",
    "        eigenvalues = np.maximum(eigenvalues, 1e-6)\n",
    "        K_inv = eigenvectors @ np.diag(1.0/eigenvalues) @ eigenvectors.T\n",
    "        mu = K_s.T @ K_inv @ y_train\n",
    "        var = np.diag(K_ss - K_s.T @ K_inv @ K_s)\n",
    "        sigma = np.sqrt(np.maximum(0, var))\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "n_samples = 10\n",
    "xt = np.linspace(0, 5, 200)\n",
    "f_x = signal_model(xt)\n",
    "\n",
    "mu, sigma = gpr(x, y, xt, rbf_kernel, sigma_f=2, l=0.5) # GPR prediction\n",
    "\n",
    "x_sorted_idx = np.argsort(x)\n",
    "x_sorted = x[x_sorted_idx]\n",
    "y_sorted = y[x_sorted_idx]\n",
    "cs = CubicSpline(x_sorted, y_sorted)\n",
    "spline_pred = cs(xt) # Cubic spline prediction, for comparison\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=xt, y=f_x, color='black', linestyle='--', alpha=0.8, ax=ax, label=r'f(x)');\n",
    "sns.scatterplot(x=x, y=y, color='red', ax=ax, label='Observations', zorder=3);\n",
    "sns.lineplot(x=xt, y=mu, color='blue', ax=ax, alpha=0.8, label=r'GPR Prediction');\n",
    "ax.fill_between(xt, mu - 2*sigma, mu + 2*sigma, color='blue', alpha=0.2, label='95% Confidence')\n",
    "sns.lineplot(x=xt, y=spline_pred, color='orange', ax=ax, alpha=0.8, label='Spline Prediction')\n",
    "ax.set_ylim(-3,3)\n",
    "ax.set_title('Gaussian Process Regression', fontsize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83aca53-d38b-4001-a029-b340dca4422f",
   "metadata": {},
   "source": [
    "Pretty good for only observing 10 points, no? Not only does it improve on simpler interpolation methods, but it has a lovely representation of uncertainty. But we start to see the real magic when we add noise.\n",
    "\n",
    "Suppose now instead we observe *noisy* samples of the function we're trying to model, $y = f(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56d326-c5f7-420f-9052-bf3f68040a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "sigma_n = 0.25 # Now with noise!\n",
    "x = rng.uniform(0, 5, n_samples)\n",
    "y = signal_model(x) + rng.normal(0.0, sigma_n, n_samples)\n",
    "\n",
    "mu, sigma = gpr(x, y, xt, rbf_kernel, sigma_n=sigma_n, sigma_f=2, l=0.5)\n",
    "\n",
    "x_sorted_idx = np.argsort(x)\n",
    "x_sorted = x[x_sorted_idx]\n",
    "y_sorted = y[x_sorted_idx]\n",
    "cs = CubicSpline(x_sorted, y_sorted)\n",
    "spline_pred = cs(xt)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=xt, y=f_x, color='black', linestyle='--', alpha=0.8, ax=ax, label=r'f(x)');\n",
    "sns.scatterplot(x=x, y=y, color='red', ax=ax, label='Noisy Observations', zorder=3);\n",
    "sns.lineplot(x=xt, y=mu, color='blue', ax=ax, alpha=0.8, label=r'GPR Prediction');\n",
    "ax.fill_between(xt, mu - 2*sigma, mu + 2*sigma, color='blue', alpha=0.2, label='95% Confidence')\n",
    "sns.lineplot(x=xt, y=spline_pred, color='orange', ax=ax, alpha=0.8, label='Spline Prediction')\n",
    "ax.set_ylim(-3,3)\n",
    "ax.set_title('Gaussian Process Regression', fontsize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870fad1-b2df-47f0-b2b3-42a6c04c86fb",
   "metadata": {},
   "source": [
    "And not only is kriging robust to noise, but those uncertainty estimates give us a way to hunt for the best *new* samples to improve our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7d71a-22f7-49a8-9a9c-c6e6048b8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_improvement(mu, sigma, f_best, xi=0.1, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Expected Improvement acquisition function\n",
    "    \n",
    "    Args:\n",
    "        mu: Mean predictions\n",
    "        sigma: Standard deviation of predictions\n",
    "        f_best: Best observed value so far\n",
    "        xi: Exploration-exploitation trade-off parameter\n",
    "        epsilon: Minimum sigma value for numerical stability\n",
    "    \"\"\"\n",
    "    imp = mu - f_best - xi\n",
    "    sigma = np.maximum(sigma, epsilon)\n",
    "    Z = imp / sigma\n",
    "    expected_imp = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return expected_imp\n",
    "\n",
    "def plot_gpr_step(x_train, y_train, iteration, sigma_n=0.0, n_iterations=10):\n",
    "    \"\"\"Helper function to plot Bayesian optimization process\"\"\"\n",
    "    x_test = np.linspace(0, 5, 200)\n",
    "    y_true = signal_model(x_test)\n",
    "    mu, sigma = gpr(x_train, y_train, x_test, rbf_kernel, sigma_n=sigma_n, sigma_f=1.0, l=0.5)\n",
    "\n",
    "    # Calculate expected improvement\n",
    "    f_best = np.max(y_train)\n",
    "    expected_imp = get_expected_improvement(mu, sigma, f_best)\n",
    "\n",
    "    # Find point with maximum expected improvement\n",
    "    next_idx = np.argmax(expected_imp)\n",
    "    x_next = x_test[next_idx]\n",
    "    if sigma_n > 0.0:\n",
    "        y_next = y_true[next_idx] + rng.normal(0.0, sigma_n)\n",
    "    else:\n",
    "        y_next = y_true[next_idx]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(x=x_test, y=y_true, color='black', linestyle='--', alpha=0.8, ax=ax, label=r'f(x)');\n",
    "    ax.scatter(x=x_train, y=y_train, color='red', label='Observations', zorder=3);\n",
    "    sns.lineplot(x=x_test, y=mu, color='blue', alpha=0.8, ax=ax, label=r'f(x)');\n",
    "    ax.fill_between(x_test, mu - 2*sigma, mu + 2*sigma, color='blue', alpha=0.2, label='95% Confidence')\n",
    "    if iteration < n_iterations:\n",
    "        ax.scatter(x=x_next, y=y_next, color='red', edgecolor='black', linewidth=2, label='Next Sample', zorder=3);\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(f\"GPR - Iteration {iteration}/{n_iterations}\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    return fig, x_next\n",
    "\n",
    "n_iterations = 10\n",
    "sigma_n = 0.0\n",
    "x_init = np.array([1.0]) # Initial observations\n",
    "y_init = signal_model(x_init) + rng.normal(0.0, sigma_n)\n",
    "x_obs = x_init.copy() # Observation set\n",
    "y_obs = y_init.copy()\n",
    "figures = []\n",
    "\n",
    "for i in range(1, n_iterations+1):\n",
    "    figs, x_next = plot_gpr_step(x_obs, y_obs, i, sigma_n=sigma_n, n_iterations=n_iterations)\n",
    "    figures.append(fig)\n",
    "    if i < n_iterations:\n",
    "        y_next = signal_model(x_next)\n",
    "        x_obs = np.concatenate((x_obs, [x_next]))\n",
    "        y_obs = np.concatenate((y_obs, [y_next]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
